{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7860d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "#os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "#os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import MBQuantSimul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8753b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasicBlock_MBQuantSimul(x, channel_size, name, strides=1, kernel_init=\"glorot_uniform\", kernel_reg = None, tau=1.0):\n",
    "    x_1 = x\n",
    "    x_2 = x\n",
    "    \n",
    "    if strides != 1:\n",
    "        #option_a\n",
    "        x_2 = tf.keras.layers.MaxPool2D(1, strides=strides, padding=\"same\", name=name+\"_sc_maxpool\"  )( tf.pad(x_2, ((0,0), (0,0), (0,0), (0, channel_size-x_2.shape[-1])), name=name+\"_sc_optionA\") )\n",
    "        #option_b\n",
    "        #x_2 = tf.keras.layers.Conv2D(channel_size, 1, strides=strides, padding='same', use_bias=False, name=name+\"_sc_conv\", kernel_initializer=INIT)(x_2)\n",
    "        #x_2 = tf.keras.layers.BatchNormalization(name=name+\"_sc_bn\")(x_2)\n",
    "\n",
    "    x_1 = MBQuantSimul.MBQuantSimulConv2D(channel_size, 3, tau=tau, strides=strides, padding=\"same\", use_bias=False, name=name+\"_conv1\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg)(x_1)\n",
    "    x_1 = tf.keras.layers.BatchNormalization(name=name+\"_bn1\")(x_1)\n",
    "    x_1 = tf.keras.layers.Activation('relu', name=name+\"_act1\")(x_1)\n",
    "    x_1 = MBQuantSimul.MBQuantActivation(tau=tau, name=name+\"_act1_quantized\")(x_1)\n",
    "    \n",
    "    x_1 = MBQuantSimul.MBQuantSimulConv2D(channel_size, 3, tau=tau, strides=1, padding=\"same\", use_bias=False, name=name+\"_conv2\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg)(x_1)\n",
    "    x_1 = tf.keras.layers.BatchNormalization(name=name+\"_bn2\")(x_1)\n",
    "    \n",
    "    x = tf.keras.layers.Add(name=name+\"_add\")([x_1, x_2])\n",
    "    x = tf.keras.layers.Activation('relu', name=name+\"_act2\")(x)\n",
    "    x = MBQuantSimul.MBQuantActivation(tau=tau, name=name+\"_act2_quantized\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def ResNet20_MBQuantSimul(input_shape=(32,32,3), classes=10, channel_sizes=16, kernel_init=\"glorot_uniform\", kernel_reg=tf.keras.regularizers.L2(1e-4)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    #pre\n",
    "    x = MBQuantSimul.MBQuantSimulConv2D(channel_sizes, 3, tau=1.0, qconfig=\"distribution_aware\", strides=1, padding=\"same\", use_bias=False, name=\"pre_conv\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name=\"pre_bn\")(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\",name=\"pre_act\")(x)\n",
    "    x = MBQuantSimul.MBQuantActivation(tau=1.0, name=\"pre_act_quantized\")(x)\n",
    "\n",
    "    #blocks_1\n",
    "    x = BasicBlock_MBQuantSimul(x, channel_sizes, \"blocks_1_1\", strides=1, tau=1.0 )\n",
    "    for i in range(1,3):\n",
    "        x = BasicBlock_MBQuantSimul(x, channel_sizes, \"blocks_1_\"+str(i+1), strides=1, tau=1.0)\n",
    "\n",
    "    #blocks_2\n",
    "    x = BasicBlock_MBQuantSimul(x, channel_sizes*2, \"blocks_2_1\", strides=2, tau=1.0)\n",
    "    for i in range(1,3):\n",
    "        x = BasicBlock_MBQuantSimul(x, channel_sizes*2, \"blocks_2_\"+str(i+1), strides=1, tau=1.0)\n",
    "\n",
    "    #blocks_3\n",
    "    x = BasicBlock_MBQuantSimul(x, channel_sizes*4, \"blocks_3_1\", strides=2, tau=1.0)\n",
    "    for i in range(1,3):\n",
    "        x = BasicBlock_MBQuantSimul(x, channel_sizes*4, \"blocks_3_\"+str(i+1), strides=1, tau=1.0)\n",
    "    \n",
    "    #pred\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"pred_gap\")(x)\n",
    "    x = MBQuantSimul.MBQuantDense(classes, name=\"pred_dense\", tau=1.0, qconfig=\"distribution_aware\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg)(x) #BIAS REGULARIZER X\n",
    "    #x = tf.keras.layers.Dense(classes, name=\"pred_dense\", kernel_initializer=kernel_init, kernel_regularizer=kernel_reg)(x) #BIAS REGULARIZER X\n",
    "    x = tf.keras.layers.Activation(\"softmax\", name=\"pred_out\")(x)\n",
    "    outputs = MBQuantSimul.MBQuantActivation(tau=1.0, name=\"pred_out_quantized\")(x)\n",
    "    \n",
    "    return MBQuantSimul.MBQuantModel(inputs=inputs, outputs=outputs, name=\"ResNet20\")\n",
    "    #return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"ResNet20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bab6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = tf.keras.models.load_model(\"BASELINE_ResNet20_CIFAR10_2.h5\")\n",
    "model_quantized = ResNet20_MBQuantSimul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c9c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)# tf.squeeze( tf.one_hot( y_train, y_train.max()+1 ) )\n",
    "y_test = tf.keras.utils.to_categorical(y_test) #tf.squeeze( tf.one_hot( y_test, y_test.max()+1 ) )\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "x_mean = np.mean(x_train, axis=(0,1,2), keepdims=True)\n",
    "x_std = np.std(x_train, axis=(0,1,2), keepdims=True)\n",
    "\n",
    "x_train = (x_train - x_mean)/x_std\n",
    "x_test = (x_test - x_mean)/x_std\n",
    "\n",
    "#x_train = tf.cast(x_train, tf.float32)\n",
    "#x_test = tf.cast(x_test, tf.float32)\n",
    "#y_train = tf.cast(y_train, tf.float32)\n",
    "#y_test = tf.cast(y_test, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804af3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_pretrained.layers:\n",
    "    if (type(layer) == tf.keras.layers.Conv2D) or (type(layer) == tf.keras.layers.Dense):\n",
    "        layer_to = model_quantized.get_layer(layer.name)\n",
    "        layer_to.kernel.assign( layer.kernel )\n",
    "        if type(layer_to.bias) != type(None):\n",
    "            layer_to.bias.assign( layer.bias )\n",
    "\n",
    "    elif type(layer) == tf.keras.layers.BatchNormalization:\n",
    "        model_quantized.get_layer(layer.name).set_weights( layer.get_weights() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9c7c68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_act_quantized tau: 23.411941528320312\n",
      "blocks_1_1_act1_quantized tau: 27.65594482421875\n",
      "blocks_1_1_act2_quantized tau: 30.236324310302734\n",
      "blocks_1_2_act1_quantized tau: 23.997173309326172\n",
      "blocks_1_2_act2_quantized tau: 48.182430267333984\n",
      "blocks_1_3_act1_quantized tau: 23.007970809936523\n",
      "blocks_1_3_act2_quantized tau: 38.236698150634766\n",
      "blocks_2_1_act1_quantized tau: 19.169614791870117\n",
      "blocks_2_1_act2_quantized tau: 27.926471710205078\n",
      "blocks_2_2_act1_quantized tau: 15.065374374389648\n",
      "blocks_2_2_act2_quantized tau: 29.794193267822266\n",
      "blocks_2_3_act1_quantized tau: 18.395614624023438\n",
      "blocks_2_3_act2_quantized tau: 49.784156799316406\n",
      "blocks_3_1_act1_quantized tau: 29.81757354736328\n",
      "blocks_3_1_act2_quantized tau: 62.60101318359375\n",
      "blocks_3_2_act1_quantized tau: 14.856278419494629\n",
      "blocks_3_2_act2_quantized tau: 63.32194137573242\n",
      "blocks_3_3_act1_quantized tau: 20.320844650268555\n",
      "blocks_3_3_act2_quantized tau: 73.78074645996094\n",
      "pred_out_quantized tau: 1.0\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(model_quantized.layers):\n",
    "    if hasattr(layer,\"qconfig\") and layer.qconfig==\"uniform\":\n",
    "        layer_name_to_find = layer.name.split(\"_quantized\")[0]\n",
    "        \n",
    "        t_calc = tf.keras.Model(inputs=model_pretrained.layers[0].input, outputs=model_pretrained.get_layer(layer_name_to_find).output )(x_train).numpy().max()\n",
    "        print(\"{} tau: {}\".format(layer.name, t_calc))\n",
    "        layer.tau.assign(t_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80260182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch %30 == 0:\n",
    "        return lr * 0.1\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "LR_Scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e396b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "391/391 [==============================] - 45s 93ms/step - loss: 0.0231 - accuracy: 0.9926 - val_loss: 0.4802 - val_accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 0.4837 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.4821 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 0.4790 - val_accuracy: 0.9201 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.4886 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9952pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.4789 - val_accuracy: 0.9183 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.4801 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9946pre_conv / 5 / 4\n",
      "pre_act_quantized / 13 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.4800 - val_accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9947pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.0178 - accuracy: 0.9947 - val_loss: 0.4875 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9946pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "391/391 [==============================] - 35s 88ms/step - loss: 0.0182 - accuracy: 0.9946 - val_loss: 0.4811 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0196 - accuracy: 0.9943 - val_loss: 0.4814 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9943pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.0190 - accuracy: 0.9943 - val_loss: 0.4855 - val_accuracy: 0.9179 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9934pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0217 - accuracy: 0.9934 - val_loss: 0.4884 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9940pre_conv / 5 / 4\n",
      "pre_act_quantized / 13 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0204 - accuracy: 0.9940 - val_loss: 0.4971 - val_accuracy: 0.9175 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9936pred_dense / 1 / 4\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0208 - accuracy: 0.9936 - val_loss: 0.5036 - val_accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9936pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.0212 - accuracy: 0.9936 - val_loss: 0.4994 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9918pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.4966 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9923pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.5031 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9917pre_conv / 13 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.5067 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9900pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.0291 - accuracy: 0.9900 - val_loss: 0.4930 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9896pre_act_quantized / 12 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0302 - accuracy: 0.9896 - val_loss: 0.4964 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9903pre_conv / 5 / 4\n",
      "pred_dense / 0 / 4\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.0307 - accuracy: 0.9903 - val_loss: 0.4974 - val_accuracy: 0.9127 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9880pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0375 - accuracy: 0.9880 - val_loss: 0.4972 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9860pre_conv / 5 / 4\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.0436 - accuracy: 0.9860 - val_loss: 0.4932 - val_accuracy: 0.9070 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9844pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0472 - accuracy: 0.9844 - val_loss: 0.4636 - val_accuracy: 0.9075 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9820pre_conv / 8 / 4\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.0550 - accuracy: 0.9820 - val_loss: 0.4799 - val_accuracy: 0.9056 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9806pre_conv / 13 / 4\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0556 - accuracy: 0.9806 - val_loss: 0.4755 - val_accuracy: 0.9058 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9746pre_conv / 5 / 4\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.0776 - accuracy: 0.9746 - val_loss: 0.4460 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9743pred_dense / 8 / 4\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0777 - accuracy: 0.9743 - val_loss: 0.4308 - val_accuracy: 0.9007 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9716pre_conv / 5 / 4\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.0903 - accuracy: 0.9716 - val_loss: 0.4152 - val_accuracy: 0.8968 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1150 - accuracy: 0.9626pre_conv / 5 / 4\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.1150 - accuracy: 0.9626 - val_loss: 0.4367 - val_accuracy: 0.8918 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9529pre_conv / 5 / 4\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1387 - accuracy: 0.9529 - val_loss: 0.4565 - val_accuracy: 0.8855 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9512pred_dense / 0 / 4\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.1480 - accuracy: 0.9512 - val_loss: 0.4652 - val_accuracy: 0.8824 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9446pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1673 - accuracy: 0.9446 - val_loss: 0.4715 - val_accuracy: 0.8775 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1899 - accuracy: 0.9358pre_conv / 5 / 4\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.1899 - accuracy: 0.9358 - val_loss: 0.4980 - val_accuracy: 0.8727 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 34s 88ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 89ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1000pre_conv / 0 / 4\n",
      "pre_conv / 1 / 4\n",
      "pre_conv / 2 / 4\n",
      "pre_conv / 3 / 4\n",
      "pre_conv / 4 / 4\n",
      "pre_conv / 5 / 4\n",
      "pre_conv / 6 / 4\n",
      "pre_conv / 7 / 4\n",
      "pre_conv / 8 / 4\n",
      "pre_conv / 9 / 4\n",
      "pre_conv / 10 / 4\n",
      "pre_conv / 11 / 4\n",
      "pre_conv / 12 / 4\n",
      "pre_conv / 13 / 4\n",
      "pre_conv / 14 / 4\n",
      "pre_conv / 15 / 4\n",
      "pre_act_quantized / 0 / 8\n",
      "pre_act_quantized / 1 / 8\n",
      "pre_act_quantized / 2 / 8\n",
      "pre_act_quantized / 3 / 8\n",
      "pre_act_quantized / 4 / 8\n",
      "pre_act_quantized / 5 / 8\n",
      "pre_act_quantized / 6 / 8\n",
      "pre_act_quantized / 7 / 8\n",
      "pre_act_quantized / 8 / 8\n",
      "pre_act_quantized / 9 / 8\n",
      "pre_act_quantized / 10 / 8\n",
      "pre_act_quantized / 11 / 8\n",
      "pre_act_quantized / 12 / 8\n",
      "pre_act_quantized / 13 / 8\n",
      "pre_act_quantized / 14 / 8\n",
      "pre_act_quantized / 15 / 8\n",
      "391/391 [==============================] - 35s 90ms/step - loss: nan - accuracy: 0.1000 - val_loss: nan - val_accuracy: 0.1000 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "335/391 [========================>.....] - ETA: 4s - loss: nan - accuracy: 0.0998"
     ]
    }
   ],
   "source": [
    "savename = \"MBQuant_ResNet20_CIFAR10\"\n",
    "checkpoint_filepath = './' + savename + '/checkpoint_finetune-{epoch}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,save_best_only=False)\n",
    "\n",
    "callback_bitwidth = MBQuantSimul.Callback_AdjustBitWidths()\n",
    "\n",
    "#model_quantized.accumulate_grads = False\n",
    "optim = tf.keras.optimizers.SGD(1e-4*10, momentum=0.9)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "model_quantized.compile(optimizer=optim, loss=loss_f, metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "history = model_quantized.fit(datagen.flow(x_train, y_train, batch_size=128), validation_data=(x_test, y_test), epochs=100, callbacks=[model_checkpoint_callback, LR_Scheduler, callback_bitwidth])\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.to_csv(savename + '.csv'.format(savename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f9753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "857c1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528e9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 21ms/step - loss: 0.4308 - accuracy: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4308093786239624, 0.9006999731063843]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finetune + LBA\n",
    "\"\"\"\n",
    "\n",
    "savename = \"MBQuant_ResNet20_CIFAR10\"\n",
    "checkpoint_filepath = './' + savename + '/checkpoint_finetune-{epoch}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,save_best_only=False)\n",
    "\n",
    "callback_bitwidth = MBQuantSimul.Callback_AdjustBitWidths()\n",
    "\n",
    "#model_quantized.accumulate_grads = False\n",
    "optim = tf.keras.optimizers.SGD(1e-4*10, momentum=0.9)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "model_quantized.compile(optimizer=optim, loss=loss_f, metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_quantized.load_weights('./' + savename + '/checkpoint_finetune-29')\n",
    "\n",
    "model_quantized.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "992476b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_avg_wb = []\n",
    "list_avg_ab = []\n",
    "for layer in model_quantized.layers[5:-3]:\n",
    "    if type(layer) == MBQuantSimul.MBQuantSimulConv2D or type(layer) == MBQuantSimul.MBQuantDense:\n",
    "        #print(layer.name)\n",
    "        #print(np.unique( layer.vec_bitwidth.numpy() ) )\n",
    "        #print(layer.vec_bitwidth.numpy().mean())\n",
    "    \n",
    "        list_avg_wb.append( layer.vec_bitwidth.numpy().mean() )\n",
    "    if type(layer) == MBQuantSimul.MBQuantActivation:\n",
    "        list_avg_ab.append( layer.vec_bitwidth.numpy().mean() )\n",
    "        \n",
    "plt.bar(x = np.arange(len(list_avg_ab)), height=list_avg_ab)\n",
    "plt.title(\"Activations\")\n",
    "plt.ylabel(\"avg. bitwidth\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.xticks([0, 4, 8, 12, 16], [1, 5, 9, 13, 17])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.bar(x = np.arange(len(list_avg_wb)), height=list_avg_wb)\n",
    "plt.title(\"Weights\")\n",
    "plt.ylabel(\"avg. bitwidth\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.xticks([0, 4, 8, 12, 16], [1, 5, 9, 13, 17])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf94f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 21ms/step - loss: 0.4821 - accuracy: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48206210136413574, 0.9208999872207642]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finetune\n",
    "\"\"\"\n",
    "\n",
    "savename = \"MBQuant_ResNet20_CIFAR10\"\n",
    "checkpoint_filepath = './' + savename + '/checkpoint_finetune-{epoch}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,save_best_only=False)\n",
    "\n",
    "callback_bitwidth = MBQuantSimul.Callback_AdjustBitWidths()\n",
    "\n",
    "#model_quantized.accumulate_grads = False\n",
    "optim = tf.keras.optimizers.SGD(1e-4*10, momentum=0.9)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "model_quantized.compile(optimizer=optim, loss=loss_f, metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_quantized.load_weights('./' + savename + '/checkpoint_finetune-3')\n",
    "\n",
    "model_quantized.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908a19fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 16ms/step - loss: 0.4003 - accuracy: 0.9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4002538025379181, 0.9072999954223633]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4bit / 32bit initial\n",
    "\"\"\"\n",
    "\n",
    "savename = \"MBQuant_ResNet20_CIFAR10\"\n",
    "checkpoint_filepath = './' + savename + '/checkpoint_finetune-{epoch}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,save_best_only=False)\n",
    "\n",
    "callback_bitwidth = MBQuantSimul.Callback_AdjustBitWidths()\n",
    "\n",
    "#model_quantized.accumulate_grads = False\n",
    "optim = tf.keras.optimizers.SGD(1e-4*10, momentum=0.9)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "model_quantized.compile(optimizer=optim, loss=loss_f, metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_quantized.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7012565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 20ms/step - loss: 0.5146 - accuracy: 0.9055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5146205425262451, 0.9054999947547913]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4bit / 8bit initial\n",
    "\"\"\"\n",
    "\n",
    "savename = \"MBQuant_ResNet20_CIFAR10\"\n",
    "checkpoint_filepath = './' + savename + '/checkpoint_finetune-{epoch}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,save_best_only=False)\n",
    "\n",
    "callback_bitwidth = MBQuantSimul.Callback_AdjustBitWidths()\n",
    "\n",
    "#model_quantized.accumulate_grads = False\n",
    "optim = tf.keras.optimizers.SGD(1e-4*10, momentum=0.9)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "model_quantized.compile(optimizer=optim, loss=loss_f, metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_quantized.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4d62ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 20ms/step - loss: 4.0581 - accuracy: 0.6383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.058104991912842, 0.6383000016212463]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4bit / 4bit initial\n",
    "\"\"\"\n",
    "\n",
    "savename = \"MBQuant_ResNet20_CIFAR10\"\n",
    "checkpoint_filepath = './' + savename + '/checkpoint_finetune-{epoch}'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,save_best_only=False)\n",
    "\n",
    "callback_bitwidth = MBQuantSimul.Callback_AdjustBitWidths()\n",
    "\n",
    "#model_quantized.accumulate_grads = False\n",
    "optim = tf.keras.optimizers.SGD(1e-4*10, momentum=0.9)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "model_quantized.compile(optimizer=optim, loss=loss_f, metrics=['accuracy'])\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_quantized.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393655c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
